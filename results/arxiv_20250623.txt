## arXiv cs.AI 논문 요약 Digest - 2025-06-23

### 1. The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making
- Authors: Abinitha Gourabathina, Yuexing Hao, Walter Gerych, Marzyeh Ghassemi
- URL: https://arxiv.org/abs/2506.17163
- 요약 (영문): clinical robustness is critical to the safe deployment of medical Large Language Models (LLMs) but key questions remain about how LLMs may differ in response to the real-world variability typified by clinical settings.
- 요약 (한글): 임상적 견고성은 의료용 대규모 언어 모델(LLM)을 안전하게 배포하는 데 매우 중요하지만, 임상 환경에서 전형적으로 나타나는 실제 변수에 따라 LLM이 어떻게 달라질 수 있는지에 대한 주요 질문이 남아 있습니다.

### 2. Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI
- Authors: Botao Zhu, Xianbin Wang, Lei Zhang, Xuemin (Sherman)Shen
- URL: https://arxiv.org/abs/2506.17130
- 요약 (영문): trust evaluation of potential collaborators has emerged as an effective mechanism for task completion . however, due to the network dynamics and varying information gathering latencies, it is extremely challenging to observe and collect all trust attributes of a collaborating device concurrently for a comprehensive trust assessment .
- 요약 (한글): 잠재적 협업자에 대한 신뢰 평가는 작업 완료를 위한 효과적인 메커니즘으로 부상했습니다. 그러나 네트워크 역학 및 다양한 정보 수집 지연 시간으로 인해 포괄적인 신뢰 평가를 위해 협업 장치의 모든 신뢰 속성을 동시에 관찰하고 수집하는 것은 매우 어렵습니다.

### 3. When Can Model-Free Reinforcement Learning be Enough for Thinking?
- Authors: Josiah P. Hanna, Nicholas E. Corrado
- URL: https://arxiv.org/abs/2506.17124
- 요약 (영문): model-free reinforcement learning (RL) trains reasoning-like capabilities . this paper seeks to build a domain-independent understanding of when RL will lead to "thinking"
- 요약 (한글): 모델 없는 강화 학습(RL)은 추론과 같은 능력을 훈련합니다. 이 백서에서는 RL이 언제 '사고'로 이어질지에 대한 도메인 독립적인 이해를 구축하고자 합니다.

### 4. Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models
- Authors: Dadi Guo, Jiayu Liu, Zhiyuan Fan, Zhitao He, Haoran Li, Yumeng Wang, Yi R. (May)Fung
- URL: https://arxiv.org/abs/2506.17114
- 요약 (영문): large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities . but the high reported accuracy of these advanced models on popular datasets often mask their true reasoning shortcomings .
- 요약 (한글): 대규모 추론 모델(예: R1, o3)은 놀라운 수학적 문제 해결 능력을 보여줬지만, 인기 있는 데이터 세트에서 이러한 고급 모델의 높은 정확도는 종종 실제 추론의 단점을 가립니다.

### 5. Are Bias Evaluation Methods Biased ?
- Authors: Lina Berrayana, Sean Rooney, Luis Garcés-Erice, Ioana Giurgiu
- URL: https://arxiv.org/abs/2506.17111
- 요약 (영문): benchmarks allow models to be compared for different aspects of safety such as toxicity, bias, harmful behavior etc. independent benchmarks adopt different approaches with distinct data sets .
- 요약 (한글): 벤치마크를 사용하면 독성, 편향성, 유해한 행동 등과 같은 안전의 다양한 측면에 대해 모델을 비교할 수 있습니다. 독립적인 벤치마크는 서로 다른 데이터 세트를 사용하여 서로 다른 접근 방식을 채택합니다.

### 6. Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving
- Authors: Chuxue Cao, Mengze Li, Juntao Dai, Jinluan Yang, Zijian Zhao, Shengyu Zhang, Weijie Shi, Chengzhong Liu, Sirui Han, Yike Guo
- URL: https://arxiv.org/abs/2506.17104
- 요약 (영문): large language models (LLMs) have shown promising first-order logic (FOL) reasoning capabilities with applications in various areas . but their effectiveness in complex mathematical reasoning involving multi-step FOL deductions is still under-research .
- 요약 (한글): 대규모 언어 모델(LLM)은 다양한 분야에 적용되어 유망한 일차 논리(FOL) 추론 기능을 보여 왔지만 다단계 FOL 추론을 포함하는 복잡한 수학적 추론에서 그 효과는 아직 연구가 진행 중입니다.

### 7. Dispositions and Roles of Generically Dependent Entities
- Authors: Fabian Neuhaus
- URL: https://arxiv.org/abs/2506.17085
- 요약 (영문): BFO 2020 does not support functions, dispositions, and roles of generically dependent continuants . in this paper, we argue that this is a severe limitation .
- 요약 (한글): 이 백서에서 우리는 이것이 심각한 한계라고 주장합니다. BFO 2020은 일반적으로 종속적인 연속체의 기능, 성향 및 역할을 지원하지 않습니다.

### 8. A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models
- Authors: Davide Frizzo, Francesco Borsatti, Gian Antonio Susto
- URL: https://arxiv.org/abs/2506.17018
- 요약 (영문): Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0 . this paper introduces a novel RUL estimation approach leveraging State Space Models (SSM) .
- 요약 (한글): 예측 유지보수(PdM)는 인더스트리 4.0 및 5.0에서 중추적인 역할을 합니다. 이 백서에서는 상태 공간 모델(SSM)을 활용하는 새로운 RUL 추정 접근 방식을 소개합니다.

### 9. Elevating Styled Mahjong Agents with Learning from Demonstration
- Authors: Lingfeng Li, Yunlong Lu, Yongyi Wang, Wenxin Li
- URL: https://arxiv.org/abs/2506.16995
- 요약 (영문): we select the Mahjong game environment as a case study . the high degree of randomness inherent in the game and the prevalence of out-of-distribution states lead to the development of highly competent bots .
- 요약 (한글): 마작 게임 환경을 사례 연구로 선택했습니다. 게임에 내재된 높은 수준의 무작위성과 분포 외 상태의 만연은 매우 유능한 봇의 개발로 이어집니다.

### 10. Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning
- Authors: Jiaqi Chen, Mingfeng Fan, Xuefeng Zhang, Jingsong Liang, Yuhong Cao, Guohua Wu, Guillaume Adrien Sartoretti
- URL: https://arxiv.org/abs/2506.16931
- 요약 (영문): task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring . we propose a multimodal fused learning framework that leverages graph and image-based representations to capture complememe .
- 요약 (한글): 작업 계획은 특히 창고 검색 및 환경 모니터링과 같은 애플리케이션에서 모바일 로봇에 필수적입니다. 우리는 그래프 및 이미지 기반 표현을 활용하여 컴플렘을 캡처하는 멀티모달 융합 학습 프레임워크를 제안합니다.

### 11. Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines
- Authors: Tomoya Kashimata, Yohei Hamakawa, Masaya Yamasaki, Kosuke Tatsumura
- URL: https://arxiv.org/abs/2506.16924
- 요약 (영문): black-box optimization algorithms and multi-armed bandit algorithms perform optimization by repeatedly taking actions and observing the corresponding instant rewards without any prior knowledge . a method using an Ising machine has been proposed to find the best action that is represented by a combination of discrete values and maximizes the instant reward in static environments .
- 요약 (한글): 블랙박스 최적화 알고리즘과 다중 무장 도적 알고리즘은 사전 지식 없이 반복적으로 행동을 취하고 그에 따른 순간 보상을 관찰하여 최적화를 수행합니다. 정적 환경에서 이산 값의 조합으로 표현되고 순간 보상을 최대화하는 최적의 행동을 찾기 위해 이싱 머신을 사용하는 방법이 제안되었습니다.

### 12. AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario
- Authors: Ciro Beneduce, Massimiliano Luca, Bruno Lepri
- URL: https://arxiv.org/abs/2506.16898
- 요약 (영문): we created 150 synthetic images for each state in the united states . we embed each image using DINO-v2 ViT-S/14 and the Fréchet Inception Distances to m .
- 요약 (한글): 미국의 각 주에 대해 150개의 합성 이미지를 만들었습니다. DINO-v2 ViT-S/14와 Fréchet 시작 거리에서 m까지의 거리를 사용하여 각 이미지를 임베드했습니다.

### 13. Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers
- Authors: Yanchen Zhu, Honghui Zou, Chufan Liu, Yuyu Luo, Yuankai Wu, Yuxuan Liang
- URL: https://arxiv.org/abs/2506.16764
- 요약 (영문): mobile chargers have emerged as a flexible solution, capable of relocating to align with demand fluctuations . this paper addresses the optimal planning and operation of hygienic charging stations .
- 요약 (한글): 이동식 충전기는 수요 변동에 맞춰 재배치할 수 있는 유연한 솔루션으로 부상했습니다. 이 백서에서는 위생적인 충전소의 최적 계획 및 운영에 대해 다룹니다.

### 14. Incentivizing High-quality Participation From Federated Learning Agents
- Authors: Jinlong Pang, Jiaheng Wei, Yifan Hua, Chen Qian, Yang Liu
- URL: https://arxiv.org/abs/2506.16731
- 요약 (영문): existing research suffers from two caveats: voluntary and unselfish participation is often assumed . but agents may opt out of the system or provide low-quality contributions without proper incentives .
- 요약 (한글): 기존 연구에는 두 가지 주의 사항이 있습니다. 자발적이고 이타적인 참여를 가정하는 경우가 많지만, 에이전트가 시스템을 거부하거나 적절한 인센티브 없이 낮은 품질의 기여를 제공할 수 있습니다.

### 15. Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics
- Authors: Kenjiro Ide, Taiga Someya, Kohei Kawaguchi, Keisuke Fujii
- URL: https://arxiv.org/abs/2506.16696
- 요약 (영문): research has proposed models based on spatial and kinematic equations . but these are computationally expensive . also, Reinforcement learning approaches use player positions and velocities .
- 요약 (한글): 연구에서는 공간 및 운동 방정식에 기반한 모델을 제안했지만 계산 비용이 많이 들며, 강화 학습 접근 방식은 선수의 위치와 속도를 사용합니다.

### 16. No Free Lunch: Rethinking Internal Feedback for LLM Reasoning
- Authors: Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He
- URL: https://arxiv.org/abs/2506.17219
- 요약 (영문): RLHF and Reinforcement Learning with Verifiable Rewards have shown strong results . they require extensive external supervision . RLIF relies solely on intrinsic model-derived signals instead of external rewards .
- 요약 (한글): 검증 가능한 보상이 있는 강화 학습과 RLHF는 강력한 결과를 보여줬지만 광범위한 외부 감독이 필요합니다. RLIF는 외부 보상 대신 내재적 모델에서 파생된 신호에만 의존합니다.

### 17. Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens
- Authors: Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, Chuang Gan
- URL: https://arxiv.org/abs/2506.17218
- 요약 (영문): vision-language models (VLMs) excel at multimodal understanding . text-only decoding forces them to verbalize visual reasoning . the heavy image-generation pre-training often hinders the reasoning ability .
- 요약 (한글): 시각-언어 모델(VLM)은 다중 모드 이해에 탁월합니다. 텍스트만 해독하면 시각적 추론을 언어화해야 합니다. 이미지 생성에 대한 과도한 사전 훈련이 추론 능력을 방해하는 경우가 많습니다.

### 18. Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation
- Authors: Xiuyu Yang, Shuhan Tan, Philipp Krähenbühl
- URL: https://arxiv.org/abs/2506.17213
- 요약 (영문): infGen is a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation . infgen automatically sat agents in a scene as the ego vehicle enters new regions.
- 요약 (한글): infGen은 인터리브 폐쇄 루프 모션 시뮬레이션과 장면 생성을 수행하는 통합된 다음 토큰 예측 모델로, 에고 차량이 새로운 영역에 진입할 때 자동으로 에이전트를 장면에 앉힙니다.

### 19. Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting
- Authors: Tianjiao Yu, Vedant Shah, Muntasir Wahed, Ying Shen, Kiet A. Nguyen, Ismini Lourentzou
- URL: https://arxiv.org/abs/2506.17212
- 요약 (영문): we introduce Part$2$GS, a framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation . it encodes articulated components with learnable attributes, enabling structured, disentangled transformations .
- 요약 (한글): 충실도 높은 지오메트리와 물리적으로 일관된 관절을 갖춘 멀티 파트 오브젝트의 관절형 디지털 트윈을 모델링하는 프레임워크인 Part$2$GS를 소개합니다. 학습 가능한 속성으로 관절형 구성 요소를 인코딩하여 구조화되고 얽힘 없는 변환을 가능하게 합니다.

### 20. Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems
- Authors: Matias Martinez, Xavier Franch
- URL: https://arxiv.org/abs/2506.17208
- 요약 (영문): SWE-Bench is a benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories .
- 요약 (한글): SWE-Bench는 12개의 인기 있는 오픈 소스 Python 리포지토리에서 채굴된 실제 이슈와 풀 리퀘스트를 사용하여 LLM 기반 복구 시스템을 평가하도록 설계된 벤치마크입니다.

### 21. Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning
- Authors: Guozheng Ma, Lu Li, Zilin Wang, Li Shen, Pierre-Luc Bacon, Dacheng Tao
- URL: https://arxiv.org/abs/2506.17204
- 요약 (영문): introducing static network sparsity alone can unlock further scaling potential beyond dense counterparts with state-of-the-art architectures . this is achieved through simple one-shot randomization .
- 요약 (한글): 정적 네트워크 희소성을 도입하는 것만으로도 최첨단 아키텍처를 갖춘 고밀도 네트워크보다 더 많은 확장 가능성을 확보할 수 있습니다. 이는 간단한 원샷 무작위화를 통해 달성할 수 있습니다.

### 22. Facial Landmark Visualization and Emotion Recognition Through Neural Networks
- Authors: Israel Juárez-Jiménez, Tiffany Guadalupe Martínez Paredes, Jesús García-Ramírez, Eric Ramos Aguilar
- URL: https://arxiv.org/abs/2506.17191
- 요약 (영문): previous studies have shown that facial images can be used to train deep learning models . but most of these studies do not include a through dataset analysis . to address this issue, we propose facial landmark box plots .
- 요약 (한글): 이전 연구에 따르면 얼굴 이미지를 사용하여 딥러닝 모델을 훈련할 수 있지만 대부분의 연구에는 데이터 세트 분석이 포함되어 있지 않습니다. 이 문제를 해결하기 위해 우리는 얼굴 랜드마크 박스 플롯을 제안합니다.

### 23. Towards AI Search Paradigm
- Authors: Yuchen Li, Hengyi Cai, Rui Kong, Xinran Chen, Jiamin Chen, Jun Yang, Haojie Zhang, Jiayi Li, Jiayi Wu, Yiqun Chen, Changle Qu, Keyi Kong, Wenwen Ye, Lixin Su, Xinyu Ma, Long Xia, Daiting Shi, Jiashu Zhao, Haoyi Xiong, Shuaiqiang Wang, Dawei Yin
- URL: https://arxiv.org/abs/2506.17188
- 요약 (영문): the AI Search Paradigm employs a modular architecture of four LLM-powered agents that dynamically adapt to the full spectrum of information needs . these agents collaborate dynamically through coordinated workflows to evaluate query and reasoning tasks .
- 요약 (한글): AI 검색 패러다임은 모든 정보 요구 사항에 동적으로 적응하는 4개의 LLM 기반 에이전트로 구성된 모듈식 아키텍처를 사용합니다. 이러한 에이전트는 조정된 워크플로우를 통해 동적으로 협업하여 쿼리 및 추론 작업을 평가합니다.

### 24. Continual Learning with Columnar Spiking Neural Networks
- Authors: Denis Larionov, Nikolay Bazenkov, Mikhail Kiselev
- URL: https://arxiv.org/abs/2506.17169
- 요약 (영문): this study investigates columnar-organized spiking neural networks . microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning .
- 요약 (한글): 이 연구는 기둥형으로 구성된 스파이킹 신경망을 조사합니다. 마이크로 컬럼은 사전 학습과 공유 구조가 부족할 때 새로운 작업에 가장 효율적으로 적응합니다.

### 25. Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network
- Authors: Mahin Montasir Afif, Abdullah Al Noman, K. M. Tahsin Kabir, Md. Mortuza Ahmmed, Md. Mostafizur Rahman, Mufti Mahmud, Md. Ashraful Babu
- URL: https://arxiv.org/abs/2506.17165
- 요약 (영문): the study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans .
- 요약 (한글): 이 연구는 GAN으로 생성된 이미지와 실제 뇌종양 MRI 이미지의 서로 다른 비율이 건강한 스캔과 종양 스캔을 분류하는 CNN의 성능에 어떤 영향을 미치는지 살펴봅니다.

### 26. Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity
- Authors: Samin Yeasar Arnob, Scott Fujimoto, Doina Precup
- URL: https://arxiv.org/abs/2506.17155
- 요약 (영문): offline RL algorithms can overfit on small datasets, resulting in poor performance . we introduce a technique based on sparsity to mitigate overfitting in offline reinforcement learning .
- 요약 (한글): 오프라인 강화 학습 알고리즘은 작은 데이터 세트에 과적합되어 성능이 저하될 수 있습니다. 오프라인 강화 학습에서 과적합을 완화하기 위해 희소성에 기반한 기법을 소개합니다.

### 27. Do We Need Large VLMs for Spotting Soccer Actions?
- Authors: Ritabrata Chakraborty, Rajatsubhra Chakraborty, Avijit Dasgupta, Sandeep Chaurasia
- URL: https://arxiv.org/abs/2506.17144
- 요약 (영문): we propose a shift from this video-centric approach to a text-based task . we posit that expert commentary provides rich, fine-grained descriptions and contextual cues .
- 요약 (한글): 동영상 중심의 접근 방식에서 텍스트 기반 작업으로 전환할 것을 제안합니다. 전문가 해설이 풍부하고 세분화된 설명과 맥락적 단서를 제공한다고 가정합니다.

### 28. MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification
- Authors: David Jacob Drexlin, Jonas Dippel, Julius Hense, Niklas Prenißl, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller
- URL: https://arxiv.org/abs/2506.17140
- 요약 (영문): if trained on overrepresented subpopulations, models struggle with less frequent patterns, leading to shortcut learning and biased predictions . large-scale foundation models have not fully eliminated this issue .
- 요약 (한글): 과도하게 대표되는 하위 모집단에 대해 학습된 경우, 모델은 빈도가 낮은 패턴으로 어려움을 겪으며 지름길 학습과 편향된 예측으로 이어집니다. 대규모 기초 모델은 이 문제를 완전히 제거하지 못했습니다.

### 29. Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models
- Authors: Michael Plainer, Hao Wu, Leon Klein, Stephan Günnemann, Frank Noé
- URL: https://arxiv.org/abs/2506.17139
- 요약 (영문): diffusion models provide generative procedure to sample equilibrium conformations and associated forces derived from the model's scores . however, using the forces for coarse-grained molecular dynamics simulations uncovers inconsistencies in the samples generated via classical diffusion inference and simulati .
- 요약 (한글): 확산 모델은 모델 점수에서 파생된 평형 컨포메이션과 관련 힘을 샘플링하는 생성 절차를 제공합니다. 그러나 거친 분자 역학 시뮬레이션에 힘을 사용하면 고전적인 확산 추론 및 시뮬레이션을 통해 생성된 샘플의 불일치를 발견할 수 있습니다.

### 30. Robust Training with Data Augmentation for Medical Imaging Classification
- Authors: Josué Martínez-Martínez, Olivia Brown, Mostafa Karami, Sheida Nabavi
- URL: https://arxiv.org/abs/2506.17133
- 요약 (영문): deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging . these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals.
- 요약 (한글): 의료 영상을 사용하여 의료 상태를 감지하고 진단하는 데 심층 신경망이 점점 더 많이 사용되고 있습니다. 이러한 모델은 적대적인 공격과 분포 변화에 매우 취약하여 진단 신뢰도에 영향을 미치고 의료 전문가 간의 신뢰를 훼손할 수 있습니다.

### 31. Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model
- Authors: Botao Zhu, Xianbin Wang
- URL: https://arxiv.org/abs/2506.17128
- 요약 (영문): trust is emerging as an effective tool to ensure successful completion of collaborative tasks within collaborative systems . however, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources .
- 요약 (한글): 신뢰는 협업 시스템 내에서 협업 작업을 성공적으로 완료하기 위한 효과적인 도구로 부상하고 있지만 분산된 장치, 복잡한 운영 환경, 동적으로 변화하는 리소스로 인해 작업 수행 중 협업자의 신뢰성을 신속하고 지속적으로 평가하는 것은 중요한 과제입니다.

### 32. MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation
- Authors: Shoubin Yu, Yue Zhang, Ziyang Wang, Jaehong Yoon, Mohit Bansal
- URL: https://arxiv.org/abs/2506.17113
- 요약 (영문): a training-free framework that performs modality- a multimodal reasoning . medical diagnosis requires precise reasoning over structured clinical tables . financial forecasting depends on interpreting plot-based data to make informed predictions .
- 요약 (한글): 의료 진단에는 구조화된 임상 표에 대한 정확한 추론이 필요하며, 재무 예측에는 플롯 기반 데이터를 해석하여 정보에 입각한 예측을 해야 합니다.

### 33. TransDreamerV3: Implanting Transformer In DreamerV3
- Authors: Shruti Sadanand Dongare, Amun Kharel, Jonathan Samuel, Xiaona Zhou
- URL: https://arxiv.org/abs/2506.17103
- 요약 (영문): TransDreamerV3 is a reinforcement learning model that enhances the DreamerV3 architecture by integrating a transformer encoder . the model is designed to improve memory and decision-making capabilities in complex environments .
- 요약 (한글): TransDreamerV3는 트랜스포머 인코더를 통합하여 DreamerV3 아키텍처를 강화한 강화 학습 모델로, 복잡한 환경에서 기억력과 의사 결정 능력을 향상시키도록 설계되었습니다.

### 34. Identifiability of Deep Polynomial Neural Networks
- Authors: Konstantin Usevich, Clara Dérand, Ricardo Borsoi, Marianne Clausel
- URL: https://arxiv.org/abs/2506.17093
- 요약 (영문): polynomial neural networks (PNNs) possess a rich algebraic and geometric structure . their identifiability remains poorly understood . our results reveal an intricate interplay between activation degrees and layer widths .
- 요약 (한글): 다항식 신경망(PNN)은 풍부한 대수적, 기하학적 구조를 가지고 있지만, 그 식별 가능성은 아직 제대로 이해되지 않았습니다. 우리의 연구 결과는 활성화 정도와 레이어 폭 사이의 복잡한 상호 작용을 밝혀냈습니다.

### 35. Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs
- Authors: Ricardo Rei, Nuno M. Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, André F. T. Martins
- URL: https://arxiv.org/abs/2506.17080
- 요약 (영문): tower+ is a suite of models designed to deliver strong performance across both tv and machine translation . this paper introduces a series of high-performance models aimed at achieving state-of-the-art performance on specific tasks .
- 요약 (한글): TOWER+는 TV와 기계 번역 모두에서 강력한 성능을 제공하도록 설계된 모델 제품군입니다. 이 백서에서는 특정 작업에서 최첨단 성능을 달성하는 것을 목표로 하는 일련의 고성능 모델을 소개합니다.

### 36. LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI
- Authors: Valeria Vuk, Cristina Sarasua, Fabrizio Gilardi
- URL: https://arxiv.org/abs/2506.17073
- 요약 (영문): a wide range of participation is essential for democracy, as it helps prevent the dominance of extreme views, erosion of legitimacy, and political polarization . an LLM-based bot can widen the scope of perspectives expressed by participants in online political discussions .
- 요약 (한글): 광범위한 참여는 극단적 견해의 지배, 정당성의 침식, 정치적 양극화를 방지하는 데 도움이 되므로 민주주의에 필수적입니다. LLM 기반 봇은 온라인 정치 토론에서 참가자가 표현하는 관점의 범위를 넓힐 수 있습니다.

### 37. Flow-Based Non-stationary Temporal Regime Causal Structure Learning
- Authors: Abdellah Rahmani, Pascal Frossard
- URL: https://arxiv.org/abs/2506.17065
- 요약 (영문): multivariate time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure . causal structure learning in this setting is challenging due to (1) non stationarit .
- 요약 (한글): 다변량 시계열은 여러 레짐, 즉 선험적으로 경계를 알 수 없는 연속적인 시간 세그먼트를 나타내며 각 레짐은 고유한 인과 구조를 갖습니다. 이 설정에서의 인과 구조 학습은 (1) 비정태적이기 때문에 어렵습니다.

### 38. From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers
- Authors: Jingtong Su, Julia Kempe, Karen Ullrich
- URL: https://arxiv.org/abs/2506.17052
- 요약 (영문): Transformers achieve state-of-the-art performance across language and vision tasks . attribution methods help advance interpretability by assigning model outputs associated with a target concept to specific model components .
- 요약 (한글): 어트리뷰션 메서드는 목표 개념과 관련된 모델 출력을 특정 모델 구성 요소에 할당하여 해석 가능성을 향상시키는 데 도움이 되며, 언어 및 시각 작업 전반에서 최첨단 성능을 구현합니다.

### 39. MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection
- Authors: Joshua Schraven, Alexander Windmann, Oliver Niggemann
- URL: https://arxiv.org/abs/2506.17041
- 요약 (영문): this paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset . a reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforcing .
- 요약 (한글): 이 백서에서는 MAWILAB v1.1 데이터 세트에서 파생된 흐름 기반 벤치마크인 MAWIFlow를 소개합니다. 원시 패킷 캡처를 흐름 표현 컨포싱으로 변환하는 재현 가능한 전처리 파이프라인이 제시됩니다.

### 40. LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation
- Authors: Elizabeth Fons, Alejandro Sztrajman, Yousef El-Laham, Luciana Ferrer, Svitlana Vyetrenko, Manuela Veloso
- URL: https://arxiv.org/abs/2506.17039
- 요약 (영문): we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data . we integrate this layer into a novel score-based diffu .
- 요약 (한글): 불규칙하게 샘플링된 데이터의 파워 스펙트럼을 안정적으로 계산할 수 있는 차별적인 Lomb--Scargle 계층을 도입하고, 이 계층을 새로운 점수 기반 디퓨저에 통합합니다.

### 41. Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning
- Authors: Giuseppe Attanasio, Sonal Sannigrahi, Ben Peters, André F. T. Martins
- URL: https://arxiv.org/abs/2506.17019
- 요약 (영문): the paper presents the IT-IST submission to the IWSLT 2025 Shared Task on Instruction Following Speech Processing . we submit results for the short track, i.e., speech recognition, translation, and spoken question answering .
- 요약 (한글): 이 논문은 음성 처리에 따른 교육에 관한 IWSLT 2025 공유 과제에 제출한 IT-IST 제출물입니다. 음성 인식, 번역 및 음성 질문 답변과 같은 쇼트 트랙에 대한 결과를 제출합니다.

### 42. TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs
- Authors: Sahil Kale, Vijaykant Nadadur
- URL: https://arxiv.org/abs/2506.16990
- 요약 (영문): laTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation . current benchmarks completely lack evaluation of this ability .
- 요약 (한글): laTeX는 조판의 정밀성과 유연성으로 인해 과학 문서 작성의 표준이 되었습니다. 현재 벤치마크에서는 이 기능에 대한 평가가 완전히 부족합니다.

### 43. Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond
- Authors: Antonin Berthon, Mihaela van der Schaar
- URL: https://arxiv.org/abs/2506.16982
- 요약 (영문): traditional Knowledge Tracing (KT) methods rely on opaque latent embeddings . even LLM-based approaches generate direct predictions or summaries .
- 요약 (한글): 기존의 지식 추적(KT) 방식은 불투명한 잠재 임베딩에 의존하며, LLM 기반 접근 방식조차도 직접적인 예측 또는 요약을 생성합니다.

### 44. Latent Concept Disentanglement in Transformer-based Language Models
- Authors: Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy
- URL: https://arxiv.org/abs/2506.16975
- 요약 (영문): large language models (LLMs) use in-context learning (ICL) to solve a new task . this begs the question of whether transformers represent latent structures as part of their computation or if they take shortcuts .
- 요약 (한글): 대규모 언어 모델(LLM)은 새로운 작업을 해결하기 위해 컨텍스트 내 학습(ICL)을 사용합니다. 이는 변환기가 계산의 일부로 잠재 구조를 나타내는지 아니면 지름길을 택하는지에 대한 의문을 제기합니다.

### 45. Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)
- Authors: Oliver Schön, Sofie Haesaert, Sadegh Soudjani
- URL: https://arxiv.org/abs/2506.16971
- 요약 (영문): the requirement for identifying accurate system representations has compromised the scalability of formal methods . the resulting models are often too complex for effective decision making with formal correctness and performance guarantees .
- 요약 (한글): 정확한 시스템 표현을 식별해야 하는 요구 사항으로 인해 공식적인 방법의 확장성이 저하되었습니다. 결과 모델이 너무 복잡하여 공식적인 정확성과 성능을 보장하는 효과적인 의사 결정을 내리기 어려운 경우가 많습니다.

### 46. Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs
- Authors: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- URL: https://arxiv.org/abs/2506.16962
- 요약 (영문): multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks . existing approaches exhibit a deficiency in offering a comprehensive framework for searching and evaluating effective reasoning paths towards critical diagnosis .
- 요약 (한글): 다중 모드 대규모 언어 모델(MLLM)은 일반적인 작업에서 강력한 추론 능력을 입증하기 시작했습니다. 기존 접근 방식은 중요한 진단을 향한 효과적인 추론 경로를 검색하고 평가하기 위한 포괄적인 프레임워크를 제공하는 데 부족함을 보였습니다.

### 47. A deep learning and machine learning approach to predict neonatal death in the context of São Paulo
- Authors: Mohon Raihan, Plabon Kumar Saha, Rajan Das Gupta, A Z M Tahmidul Kabir, Afia Anjum Tamanna, Md. Harun-Ur-Rashid, Adnan Bin Abdus Salam, Md Tanvir Anjum, A Z M Ahteshamul Kabir
- URL: https://arxiv.org/abs/2506.16929
- 요약 (영문): worldwide data indicate that 26.693 babies out of 1,000 births die . early prediction of endangered babies is crucial . machine learning was used to determine whether a newborn baby is at risk .
- 요약 (한글): 전 세계 데이터에 따르면 1,000명 중 26.693명의 아기가 사망합니다 . 위험에 처한 아기의 조기 예측이 중요합니다 . 신생아의 위험 여부를 판단하는 데 머신 러닝이 사용되었습니다 .

### 48. Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence
- Authors: Jack Griffiths, Steven A. Wrathmall, Simon A. Gardiner
- URL: https://arxiv.org/abs/2506.16925
- 요약 (영문): we demonstrate an artificial intelligence approach for rapid, non-destructive estimation of the chemical potential and temperature from single-shot, in situ imaged density profiles of finite-temperature Bose gases .
- 요약 (한글): 유한 온도 보스 가스의 단일 샷, 현장 이미지 밀도 프로파일에서 화학 전위 및 온도를 비파괴적으로 신속하게 추정하는 인공지능 접근 방식을 시연합니다.

### 49. Towards Effective Complementary Security Analysis using Large Language Models
- Authors: Jonas Wagner, Simon Müller, Christian Näther, Jan-Philipp Steghöfer, Andreas Both
- URL: https://arxiv.org/abs/2506.16899
- 요약 (영문): a key challenge in security analysis is the manual evaluation of potential security weaknesses generated by static application security testing tools . we propose using Large Language Models (LLMs) to improve the assessment of SAST findings .
- 요약 (한글): 보안 분석의 핵심 과제는 정적 애플리케이션 보안 테스트 도구에서 생성된 잠재적 보안 취약점을 수동으로 평가하는 것입니다. 저희는 대규모 언어 모델(LLM)을 사용하여 SAST 결과의 평가를 개선할 것을 제안합니다.

### 50. With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You
- Authors: Fabian Gröger, Shuo Wen, Huyen Le, Maria Brbić
- URL: https://arxiv.org/abs/2506.16895
- 요약 (영문): multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval . existing models typically rely on millions of paired multimodal samples .
- 요약 (한글): 멀티모달 모델은 제로 샷 분류 및 교차 모드 검색을 포함한 멀티모달 정렬이 필요한 복잡한 작업에서 강력한 기능을 입증했습니다. 기존 모델은 일반적으로 수백만 개의 쌍을 이루는 멀티모달 샘플에 의존합니다.

### 51. The Importance of Being Lazy: Scaling Limits of Continual Learning
- Authors: Jacopo Graldi, Alessandro Breccia, Giulia Lanzillotta, Thomas Hofmann, Lorenzo Noci
- URL: https://arxiv.org/abs/2506.16884
- 요약 (영문): despite recent efforts, neural networks still struggle to learn in non-stationary environments, and our understanding of catastrophic forgetting (CF) is far from complete . we reconcile existing contradictory observations on scale in the literature .
- 요약 (한글): 최근의 노력에도 불구하고 신경망은 여전히 고정되지 않은 환경에서 학습하는 데 어려움을 겪고 있으며, 치명적 망각(CF)에 대한 우리의 이해는 아직 완전하지 않습니다. 우리는 문헌에서 규모에 대한 기존의 모순된 관찰을 조정합니다.

### 52. ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control
- Authors: Jun Fu, Bin Tian, Haonan Chen, Shi Meng, Tingting Yao
- URL: https://arxiv.org/abs/2506.16856
- 요약 (영문): human drivers demonstrate the ability to park intuitively without explicit modeling . we propose a Transformer-based end-to-end framework for autonomous parking that learns from experts .
- 요약 (한글): 인간 운전자는 명시적인 모델링 없이도 직관적으로 주차할 수 있는 능력을 보여줍니다. 전문가로부터 학습하는 자율 주차를 위한 트랜스포머 기반 엔드투엔드 프레임워크를 제안합니다.

### 53. Bandwidth Selectors on Semiparametric Bayesian Networks
- Authors: Victor Alejandre (1), Concha Bielza (1), Pedro Larrañaga (1) ((1) Universidad Politecnica de Madrid, Spain)
- URL: https://arxiv.org/abs/2506.16844
- 요약 (영문): semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models . kernel density estimators (KDEs) are employed to learn the bandwidth matrix for the KDEs .
- 요약 (한글): 준모수적 베이지안 네트워크(SPBN)는 모수적 및 비모수적 확률 모델을 통합하고, 커널 밀도 추정기(KDE)는 KDE의 대역폭 행렬을 학습하는 데 사용되며, 반모수적 베이지안 네트워크(SPBN)는 모수적 및 비모수적 확률 모델을 통합하고, 커널 밀도 추정기(KDE)는 커널 밀도 추정기(KDE)의 대역폭 행렬을 학습하는 데 사용됩니다.

### 54. AnyTraverse: An off-road traversability framework with VLM and human operator in the loop
- Authors: Sattwik Sahu, Agamdeep Singh, Karthik Nambiar, Srikanth Saripalli, P.B. Sujit
- URL: https://arxiv.org/abs/2506.16826
- 요약 (영문): current frameworks struggle due to significant variations in unstructured environments and uncertain scene changes . anyTraverse combines natural language-based prompts with human-operator assistance to determine navigable regions .
- 요약 (한글): 현재의 프레임워크는 구조화되지 않은 환경과 불확실한 장면의 변화로 인해 어려움을 겪습니다. 애니트래버스는 자연어 기반 프롬프트와 작업자 지원을 결합하여 탐색 가능한 영역을 결정합니다.

### 55. Learning Dexterous Object Handover
- Authors: Daniel Frau-Alfaro, Julio Castaño-Amoros, Santiago Puente, Pablo Gil, Roberto Calandra
- URL: https://arxiv.org/abs/2506.16822
- 요약 (영문): we demonstrate the use of Reinforcement Learning (RL) for dexterous object handover between two multi-finger hands . key to this task is a novel reward function based on dual quaternions to minimize rotation distance .
- 요약 (한글): 두 개의 멀티 핑거 손 사이에서 능숙한 물체 핸드오버를 위해 강화 학습(RL)을 사용하는 방법을 시연합니다. 이 작업의 핵심은 회전 거리를 최소화하는 이중 쿼터니언 기반의 새로운 보상 기능입니다.

### 56. Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection
- Authors: Yuchu Jiang, Jiaming Chu, Jian Zhao, Xin Zhang, Xu Yang, Lei Jin, Chi Zhang, Xuelong Li
- URL: https://arxiv.org/abs/2506.16819
- 요약 (영문): existing deepfake detection methods primarily target image-level classification or pixel-wise localization . some achieve high accuracy but often suffer from limited generalization across manipulation types .
- 요약 (한글): 기존의 딥페이크 탐지 방법은 주로 이미지 수준의 분류 또는 픽셀 단위의 로컬라이제이션을 대상으로 하며, 일부는 높은 정확도를 달성하지만 조작 유형에 따라 일반화가 제한되는 경우가 많습니다.

### 57. Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning
- Authors: Chengpeng Hu, Ziming Wang, Bo Yuan, Jialin Liu, Chengqi Zhang, Xin Yao
- URL: https://arxiv.org/abs/2506.16795
- 요약 (영문): dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time . in real-world scenarios, historical task records are usually available .
- 요약 (한글): 동적 자재 취급(DMH)은 동적으로 도착하는 자재 운송 작업을 적합한 차량에 실시간으로 할당하는 작업으로, 실제 시나리오에서는 일반적으로 과거 작업 기록을 사용할 수 있습니다.

### 58. MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning
- Authors: Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Meng Han
- URL: https://arxiv.org/abs/2506.16792
- 요약 (영문): despite efforts to align large language models with societal and moral values, these models remain susceptible to jailbreak attacks . we propose an effective method for jailbreaking black-box large language Models via iterative semantic Tuning .
- 요약 (한글): 대규모 언어 모델을 사회적, 도덕적 가치와 일치시키려는 노력에도 불구하고 이러한 모델은 여전히 탈옥 공격에 취약합니다. 우리는 반복적인 시맨틱 튜닝을 통해 블랙박스 대규모 언어 모델을 탈옥하는 효과적인 방법을 제안합니다.

### 59. TabArena: A Living Benchmark for Machine Learning on Tabular Data
- Authors: Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, and David Salinas, Frank Hutter
- URL: https://arxiv.org/abs/2506.16791
- 요약 (영문): the need for standardized and reliable benchmarks is higher than ever . current benchmarks are static . they are not updated even if flaws are discovered .
- 요약 (한글): 표준화되고 신뢰할 수 있는 벤치마크의 필요성이 그 어느 때보다 높습니다. 현재 벤치마크는 정적입니다. 결함이 발견되어도 업데이트되지 않습니다.

### 60. What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity
- Authors: Youjin Kong
- URL: https://arxiv.org/abs/2506.16782
- 요약 (영문): fair-ML research implicitly appeals to distributive equality . unfair ML models are seen as wrong because they unequally distribute such benefits .
- 요약 (한글): 공정한 ML 연구는 암묵적으로 분배 평등에 호소합니다. 불공정한 ML 모델은 그러한 혜택을 불평등하게 분배하기 때문에 잘못된 것으로 간주됩니다.

### 61. PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model
- Authors: Beomseok Ko, Hyeryung Jang
- URL: https://arxiv.org/abs/2506.16776
- 요약 (영문): naive compression models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes . in this paper, we propose a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD)
- 요약 (한글): 네이티브 압축 모델은 이미지 생성에는 탁월하지만 반복적인 마르코프 체인 프로세스에 의존하기 때문에 계산 및 리소스 집약적입니다. 이 백서에서는 프로그레시브 정량화(PQ)와 보정 지원 증류(CAD)를 결합한 새로운 하이브리드 압축 프레임워크를 제안합니다.

### 62. Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly
- Authors: Lance Ying, Ryan Truong, Katherine M. Collins, Cedegao E. Zhang, Megan Wei, Tyler Brooke-Wilson, Tan Zhi-Xuan, Lionel Wong, Joshua B. Tenenbaum
- URL: https://arxiv.org/abs/2506.16755
- 요약 (영문): language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily observed . we propose language-informed Rational Agent Synthesis (LIRAS) framework for drawing context-specific social inferences .
- 요약 (한글): 언어는 환경 역학에 대한 추상적인 정보와 쉽게 관찰할 수 없는 에이전트에 대한 구체적인 정보를 모두 제공할 수 있습니다. 우리는 상황에 맞는 사회적 추론을 도출하기 위한 언어 정보 기반 합리적 에이전트 합성(LIRAS) 프레임워크를 제안합니다.

### 63. Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding
- Authors: Jongmin Park, Seunghoon Han, Won-Yong Shin, Sungsu Lim
- URL: https://arxiv.org/abs/2506.16754
- 요약 (영문): heterogeneous graph embedding models rely on a single hyperbolic space . this approach may fail to capture the diverse power-law structures . to address this limitation, we propose a metapath-based model .
- 요약 (한글): 이기종 그래프 임베딩 모델은 단일 쌍곡선 공간에 의존합니다. 이러한 접근 방식은 다양한 전력법 구조를 포착하지 못할 수 있습니다. 이러한 한계를 해결하기 위해 메타패스 기반 모델을 제안합니다.

### 64. Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation
- Authors: Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, Shin Ishii
- URL: https://arxiv.org/abs/2506.16753
- 요약 (영문): reinforcement learning (RL) methods have been shown to handle adversarial input observations . this process introduces mutual dependencies between the ag and the agent .
- 요약 (한글): 강화 학습(RL) 방법은 적대적 입력 관찰을 처리하는 것으로 나타났습니다. 이 과정에서 에이전트와 에이전트 간에 상호 의존성이 도입됩니다.

### 65. RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching
- Authors: Hyun Joon Park, Jeongmin Liu, Jin Sob Kim, Jeong Yeol Yang, Sung Won Han, Eunwoo Song
- URL: https://arxiv.org/abs/2506.16741
- 요약 (영문): we introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model . it leverages velocity consistency constraints in flow matching training .
- 요약 (한글): 빠르고 충실도가 높은 TTS 음향 모델인 랩플로우-TTS를 소개합니다. 이 모델은 플로우 매칭 훈련에서 속도 일관성 제약 조건을 활용합니다.

### 66. LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization
- Authors: Daejin Jo, Jeeyoung Yun, Byungseok Roh, Sungwoong Kim
- URL: https://arxiv.org/abs/2506.16738
- 요약 (영문): discrete speech tokens have emerged as a core interface between speech and text . recent approaches aim to isolate semantic information from low-level acoustics . previous methods use SSL teachers such as HuBERT to extract semantic representations .
- 요약 (한글): 개별 음성 토큰은 음성과 텍스트 사이의 핵심 인터페이스로 부상했습니다. 최근의 접근 방식은 저수준 음향에서 의미 정보를 분리하는 것을 목표로 합니다. 이전 방법은 HuBERT와 같은 SSL 교사를 사용하여 의미 표현을 추출합니다.

### 67. On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis
- Authors: Fanchen Bu, Kijung Shin
- URL: https://arxiv.org/abs/2506.16732
- 요약 (영문): unsupervised combinatorial optimization (UCO) aims to have continuous decisions that are promising in a probabilistic sense for each training instance . enables end-to-end training on initially discrete and non-differentiable problems .
- 요약 (한글): 비지도 조합 최적화(UCO)는 각 훈련 인스턴스에 대해 확률적 의미에서 유망한 연속적인 결정을 내리는 것을 목표로 하며, 초기에 이산적이고 비분화 가능한 문제에 대한 엔드투엔드 훈련을 가능하게 합니다.

### 68. The Role of Model Confidence on Bias Effects in Measured Uncertainties
- Authors: Xinyi Liu, Weiguang Wang, Hangfeng He
- URL: https://arxiv.org/abs/2506.16724
- 요약 (영문): large language models are becoming increasingly popular for open-ended tasks . assessing epistemic uncertainty is challenging due to the presence of aleatoric uncertainty .
- 요약 (한글): 개방형 작업에서 대규모 언어 모델이 점점 더 인기를 얻고 있습니다. 인식론적 불확실성을 평가하는 것은 알레토릭 불확실성의 존재로 인해 어렵습니다.

### 69. TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data
- Authors: Yuping Yan, Yizhi Wang, Yuanshuai Li, Yaochu Jin
- URL: https://arxiv.org/abs/2506.16723
- 요약 (영문): serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead . however, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks .
- 요약 (한글): 직렬 파이프라인 학습은 낮은 통신 오버헤드로 사일로 간 연합 학습에서 데이터 이질성을 처리하는 효율적인 패러다임이지만, 클라이언트 간에 모델을 직접 전송하면 개인정보 보호 규정을 위반할 수 있으며 그라데이션 유출 및 연결 공격에 취약할 수 있습니다.

### 70. Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation
- Authors: Chenxu Wang, Yonggang Jin, Cheng Hu, Youpeng Zhao, Zipeng Dai, Jian Zhao, Shiyu Huang, Liuyu Xiang, Junge Zhang, Zhaofeng He
- URL: https://arxiv.org/abs/2506.16718
- 요약 (영문): Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents . we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA)
- 요약 (한글): 단일 에이전트를 새로운 멀티 에이전트 시스템에 적응시키려면 다양한 작업, 환경, 낯선 팀원 및 상대방과의 상호작용에 대한 조정이 필요하기 때문에 보다 포괄적인 설정인 에이전트 협업-경쟁 적응(ACCA)을 제안합니다.

### 71. ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models
- Authors: Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao
- URL: https://arxiv.org/abs/2506.16712
- 요약 (영문): Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences . this often results in incomplete or overly speculative reasoning paths .
- 요약 (한글): 생성적 보상 모델(GRM)은 스칼라 보상 모델보다 인간의 선호도를 파악하는 데 더 큰 유연성을 제공하지만, 종종 불완전하거나 지나치게 추측적인 추론 경로를 초래할 수 있습니다.

### 72. Large Language Models as Psychological Simulators: A Methodological Guide
- Authors: Zhicheng Lin
- URL: https://arxiv.org/abs/2506.16702
- 요약 (영문): this article provides a framework for using LLMs as psychological simulators across two primary applications . we present methods for developing psychologically grounded personas that move beyond demographic categories .
- 요약 (한글): 이 글에서는 두 가지 주요 애플리케이션에서 LLM을 심리 시뮬레이터로 사용하기 위한 프레임워크를 제공합니다. 인구통계학적 범주를 넘어서는 심리학적 근거를 가진 페르소나를 개발하는 방법을 제시합니다.

### 73. From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology
- Authors: Zhicheng Lin
- URL: https://arxiv.org/abs/2506.16697
- 요약 (영문): large language models (LLMs) are rapidly being adopted across psychology, serving as research tools, experimental subjects, human simulators, and computational models of cognition . however, the application of human measurement tools to these systems can produce contradictory results .
- 요약 (한글): 대규모 언어 모델(LLM)은 심리학 전반에 걸쳐 연구 도구, 실험 대상, 인간 시뮬레이터 및 인지 계산 모델로 빠르게 채택되고 있지만, 이러한 시스템에 인간 측정 도구를 적용하면 모순된 결과를 낳을 수 있습니다.

### 74. Fast and Stable Diffusion Planning through Variational Adaptive Weighting
- Authors: Zhiying Qiu, Tao Lin
- URL: https://arxiv.org/abs/2506.16688
- 요약 (영문): Diffusion models have recently shown promise in offline RL . however, these methods often suffer from high training costs and slow convergence . existing loss weighting functions typically rely on neural network approxim .
- 요약 (한글): 확산 모델은 최근 오프라인 RL에서 가능성을 보였지만, 이러한 방법은 종종 높은 훈련 비용과 느린 수렴으로 어려움을 겪습니다. 기존 손실 가중치 함수는 일반적으로 신경망 근사값에 의존합니다.

### 75. A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation
- Authors: Penglong Zhai, Yifang Yuan, Fanyi Di, Jie Li, Yue Liu, Chen Li, Jie Huang, Sicong Wang, Yao Xu, Xin Li
- URL: https://arxiv.org/abs/2506.16683
- 요약 (영문): new research has explored the use of semantic tokens as an alternative to ID tokens to quantize content . this is due to the redundancy and sheer scale of the token space .
- 요약 (한글): 새로운 연구에서는 콘텐츠를 정량화하기 위해 ID 토큰의 대안으로 시맨틱 토큰을 사용하는 방법을 탐구했습니다. 이는 토큰 공간의 중복성과 엄청난 규모 때문입니다.

### 76. How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions
- Authors: Manuel Brack, Sudeep Katakol, Felix Friedrich, Patrick Schramowski, Hareesh Ravi, Kristian Kersting, Ajinkya Kale
- URL: https://arxiv.org/abs/2506.16679
- 요약 (영문): training data is at the core of any successful text-to-image models . the quality and descriptiveness of image text are crucial to a model's performance .
- 요약 (한글): 학습 데이터는 성공적인 텍스트-이미지 모델의 핵심입니다. 이미지 텍스트의 품질과 설명력은 모델의 성능에 매우 중요합니다.

### 77. A Minimalist Optimizer Design for LLM Pretraining
- Authors: Athanasios Glentis, Jiaxiang Li, Andi Han, Mingyi Hong
- URL: https://arxiv.org/abs/2506.16659
- 요약 (영문): adaptive optimizers such as Adam require significant memory to maintain first- and second-moment matrices . a fundamental question remains: what is the minimal amount of optimizer state that is truly necessary to retain state-of-the-art performance in LLM pretraining?
- 요약 (한글): Adam과 같은 적응형 옵티마이저는 첫 번째 및 두 번째 순간 행렬을 유지하는 데 상당한 메모리가 필요하며, LLM 사전 학습에서 최신 성능을 유지하는 데 진정으로 필요한 최소한의 옵티마이저 상태는 무엇인가라는 근본적인 질문이 남아 있습니다.
