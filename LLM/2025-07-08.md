# [arXiv Digest] 2025-07-08


## 1. When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
- **Authors:** Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah
- **URL:** https://arxiv.org/abs/2507.05246
- **요약 (영문):** chain-of-thought (CoT) monitoring is an appealing AI safety defense . recent work on "unfaithfulness" has cast doubt on its reliability .
- **요약 (한글):** 생각의 사슬(CoT) 모니터링은 매력적인 AI 안전 방어 수단입니다. 최근 '불성실성'에 대한 연구로 인해 그 신뢰성에 의문이 제기되고 있습니다.

## 2. MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction
- **Authors:** Kaleem Ullah Qasim, Jiashu Zhang
- **URL:** https://arxiv.org/abs/2507.04893
- **요약 (영문):** accident severity prediction plays a critical role in transportation safety systems but is a persistently difficult task due to incomplete data, strong feature dependencies and severe class imbalance . existing methods often rely on monolithic models or black box prompting, which struggle to scale in noisy, real-world settings .
- **요약 (한글):** 사고 심각도 예측은 교통 안전 시스템에서 중요한 역할을 하지만 불완전한 데이터, 강력한 기능 종속성, 심각한 등급 불균형으로 인해 지속적으로 어려운 작업입니다. 기존 방법은 종종 모놀리식 모델이나 블랙박스 프롬프트에 의존하는데, 이는 잡음이 많은 실제 환경에서 확장하기 어렵습니다.

## 3. DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine
- **Authors:** Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang
- **URL:** https://arxiv.org/abs/2507.04877
- **요약 (영문):** current large language models exhibit notable limitations in medical applications, particularly in conducting effective multi-turn dialogues and proactive questioning . these shortcomings hinder practical application and effectiveness in simulating real-world diagnostic scenarios .
- **요약 (한글):** 현재의 대규모 언어 모델은 의료 애플리케이션, 특히 효과적인 멀티턴 대화 및 사전 질문 수행에 있어 현저한 한계를 보입니다. 이러한 단점은 실제 진단 시나리오를 시뮬레이션할 때 실제 적용과 효과를 저해합니다.

## 4. Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents
- **Authors:** George Jagadeesh, Srikrishna Iyer, Michal Polanowski, Kai Xin Thia
- **URL:** https://arxiv.org/abs/2507.04803
- **요약 (영문):** this study examines the feasibility of applying large language models for forecasting the impact of traffic incidents on the traffic flow . the use of LLMs has several advantages over existing machine learning-based solutions such as not requiring a large training dataset and the ability to utilize incident logs .
- **요약 (한글):** 이 연구는 교통 사고가 교통 흐름에 미치는 영향을 예측하기 위한 대규모 언어 모델 적용의 타당성을 검토합니다. LLM을 사용하면 대규모 학습 데이터 세트가 필요하지 않고 사고 로그를 활용할 수 있는 등 기존 머신 러닝 기반 솔루션에 비해 몇 가지 장점이 있습니다.

## 5. FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System
- **Authors:** Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen
- **URL:** https://arxiv.org/abs/2507.04770
- **요약 (영문):** we propose a multi-agent system for automatic furniture decoration . given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests automating the decoration process.
- **요약 (한글):** 우리는 자동 가구 장식을 위한 다중 에이전트 시스템을 제안합니다. 사람의 프롬프트와 작업용 책상이나 TV 스탠드와 같은 가정용 가구 품목이 주어지면 우리 시스템은 장식 프로세스를 자동화할 것을 제안합니다.

## 6. LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction
- **Authors:** Sungmin Lee, Minju Kang, Joonhee Lee, Seungyong Lee, Dongju Kim, Jingi Hong, Jun Shin, Pei Zhang, JeongGil Ko
- **URL:** https://arxiv.org/abs/2507.04748
- **요약 (영문):** QA interfaces powered by large language models (LLMs) present a promising direction for improving interactivity with HVAC systems . enabling accurate, real-time, and context-aware interactions introduces unique challenges, including the integration of frequently updated sensor data .
- **요약 (한글):** 대규모 언어 모델(LLM)로 구동되는 QA 인터페이스는 HVAC 시스템과의 상호 작용을 개선하기 위한 유망한 방향을 제시합니다. 정확한 실시간 상황 인식 상호 작용을 구현하려면 자주 업데이트되는 센서 데이터의 통합을 비롯한 고유한 과제가 발생합니다.

## 7. Activation Steering for Chain-of-Thought Compression
- **Authors:** Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram
- **URL:** https://arxiv.org/abs/2507.04742
- **요약 (영문):** large language models excel at complex reasoning when they include intermediate steps . verbose, English-heavy CoTs and concise, math-centric coTs occupy distinct regions in the model's residual-stream activation space .
- **요약 (한글):** 대규모 언어 모델은 중간 단계를 포함할 때 복잡한 추론에 탁월합니다. 장황한 영어 중심의 CoT와 간결하고 수학 중심적인 CoT는 모델의 잔류 스트림 활성화 공간에서 서로 다른 영역을 차지합니다.

## 8. ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning
- **Authors:** Zhirong Chen, Kaiyan Chang, Zhuolin Li, Xinyang He, Chujie Chen, Cangyuan Li, Mengdi Wang, Haobo Xu, Yinhe Han, Ying Wang
- **URL:** https://arxiv.org/abs/2507.04736
- **요약 (영문):** large language models (LLMs) show significant potential for automating RTL code generation . but current approaches face a critical challenge: they can not simultaneously optimize for functional correctness and hardware quality . post-processing techniques that attempt to improve PP can improve performance .
- **요약 (한글):** 대규모 언어 모델(LLM)은 RTL 코드 생성 자동화에 상당한 잠재력을 보이지만 현재의 접근 방식은 기능적 정확성과 하드웨어 품질을 동시에 최적화할 수 없다는 중요한 과제에 직면해 있습니다. PP를 개선하려는 후처리 기술은 성능을 향상시킬 수 있습니다.

## 9. Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message
- **Authors:** Wei Duan, Li Qian
- **URL:** https://arxiv.org/abs/2507.04673
- **요약 (영문):** the rise of conversational interfaces has greatly enhanced usability . this reliance introduces an unexplored attack surface . a malicious payload is injected into a model-attributed message .
- **요약 (한글):** 대화형 인터페이스의 등장으로 사용성이 크게 향상되었습니다. 이러한 의존성은 미개척 공격 표면을 도입합니다. 악성 페이로드가 모델 어트리뷰션 메시지에 주입됩니다.

## 10. Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?
- **Authors:** Yun Qu, Qi Cheems Wang, Yixiu Mao, Vincent Tao Hu, Xiangyang Ji
- **URL:** https://arxiv.org/abs/2507.04632
- **요약 (영문):** recent advances have witnessed the effectiveness of reinforcement learning (RL) finetuning in enhancing the reasoning capabilities of large language models (LLMs) the optimization process often requires numerous iterations to achieve satisfactory performance .
- **요약 (한글):** 최근의 발전으로 대규모 언어 모델(LLM)의 추론 능력을 향상시키는 데 있어 강화 학습(RL) 미세 조정의 효과가 입증되었습니다. 최적화 프로세스에는 만족스러운 성능을 달성하기 위해 수많은 반복이 필요한 경우가 많습니다.

## 11. Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions
- **Authors:** Yuanzhe Hu, Yu Wang, Julian McAuley
- **URL:** https://arxiv.org/abs/2507.05257
- **요약 (영문):** benchmarks for LLM agents focus on evaluating reasoning, planning, and execution capabilities . another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information, is under-evaluated .
- **요약 (한글):** LLM 에이전트에 대한 벤치마크는 추론, 계획 및 실행 능력을 평가하는 데 중점을 두고 있습니다. 또 다른 중요한 요소인 에이전트가 장기 정보를 암기, 업데이트 및 검색하는 방법을 포괄하는 메모리는 저평가되어 있습니다.

## 12. All in One: Visual-Description-Guided Unified Point Cloud Segmentation
- **Authors:** Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer
- **URL:** https://arxiv.org/abs/2507.05211
- **요약 (영문):** unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained objects in complex environments . to address these challenges, we propose VDG-Uni3DSeg, a novel framework .
- **요약 (한글):** 3D 포인트 클라우드의 통합된 분할은 장면 이해에 매우 중요하지만, 희박한 구조와 제한된 주석, 복잡한 환경에서 세분화된 오브젝트를 구별하는 데 어려움을 겪습니다. 이러한 문제를 해결하기 위해 유니티는 새로운 프레임워크인 VDG-Uni3DSeg를 제안합니다.

## 13. Train-before-Test Harmonizes Language Model Rankings
- **Authors:** Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt
- **URL:** https://arxiv.org/abs/2507.05195
- **요약 (영문):** conflicting rankings hamper model selection, clouds comparisons, and adds confusion to growing ecosystem of competing models . a candidate solution to the problem is train on the test task .
- **요약 (한글):** 상충되는 순위는 모델 선택을 방해하고, 비교를 흐리게 하며, 경쟁 모델의 생태계가 성장함에 따라 혼란을 가중시킵니다. 이 문제에 대한 후보 솔루션은 테스트 작업에 대한 훈련입니다.

## 14. CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale
- **Authors:** Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen
- **URL:** https://arxiv.org/abs/2507.05178
- **요약 (영문):** despite rapid progress in large language model (LLM)-based multi-agent systems, current benchmarks fall short in evaluating their scalability, robustness, and coordination capabilities . existing environments typically focus on small-scale, fully observable, or low-complexity domains .
- **요약 (한글):** 대규모 언어 모델(LLM) 기반 다중 에이전트 시스템의 빠른 발전에도 불구하고 현재 벤치마크는 확장성, 견고성 및 조정 기능을 평가하는 데 부족합니다. 기존 환경은 일반적으로 소규모, 완전히 관찰 가능하거나 복잡성이 낮은 도메인에 초점을 맞추고 있습니다.

## 15. OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model
- **Authors:** Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang
- **URL:** https://arxiv.org/abs/2507.05177
- **요약 (영문):** the most powerful empathetic LSLMs are closed off, leaving the crucial details about the architecture, data and development opaque to researchers . openS2S is a fully open-source, transparent and transparent .
- **요약 (한글):** 가장 강력한 공감형 LSLM은 폐쇄적이어서 아키텍처, 데이터 및 개발에 대한 중요한 세부 사항을 연구자에게 불투명하게 남겨두고 있습니다. openS2S는 완전 오픈 소스이며 투명하고 투명한 .

## 16. AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models
- **Authors:** Chinnappa Guggilla, Budhaditya Roy, Trupti Ramdas Chavan, Abdul Rahman, Edward Bowen
- **URL:** https://arxiv.org/abs/2507.05157
- **요약 (영문):** Large Language Models (LLMs) adapt to various styles and genres . they produce content that is both grammatically correct and semantically meaningful . recently, they have been misused to create highly realistic phishing emails, spread fake news, generate code to automate cyber crime .
- **요약 (한글):** LLM(대규모 언어 모델)은 다양한 스타일과 장르에 적응하며 문법적으로 정확하고 의미적으로 의미 있는 콘텐츠를 생성하며 최근에는 매우 사실적인 피싱 이메일 생성, 가짜 뉴스 확산, 사이버 범죄 자동화를 위한 코드 생성에 악용되고 있습니다.

## 17. Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization
- **Authors:** Jaewook Lee, Alexander Scarlatos, Andrew Lan
- **URL:** https://arxiv.org/abs/2507.05137
- **요약 (영문):** Japanese combines syllabaries like hiragana with kanji, which are logographic characters of Chinese origin . keywords mnemonics are a common strategy to aid memorization .
- **요약 (한글):** 일본어는 히라가나와 같은 음절과 한자, 즉 한자에서 유래한 문자를 결합한 언어입니다. 키워드 니모닉은 암기를 돕기 위한 일반적인 전략입니다.

## 18. An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques
- **Authors:** Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz
- **URL:** https://arxiv.org/abs/2507.05123
- **요약 (영문):** Large Language Models (LLMs) continue to advance natural language processing . their ability to generate human-like text has not been comprehensively evaluated .
- **요약 (한글):** 대규모 언어 모델(LLM)은 자연어 처리를 지속적으로 발전시키고 있지만, 인간과 유사한 텍스트를 생성하는 능력은 종합적으로 평가되지 않았습니다.

## 19. LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks
- **Authors:** Jiajia Guo, Peiwen Jiang, Chao-Kai Wen, Shi Jin, Jun Zhang
- **URL:** https://arxiv.org/abs/2507.05121
- **요약 (영문):** Accurate channel state information (CSI) is critical to wireless communication systems . existing methods depend on task-specific neural networks (NNs) that require expert-driven design and large training datasets .
- **요약 (한글):** 정확한 채널 상태 정보(CSI)는 무선 통신 시스템에 매우 중요하며, 기존 방법은 전문가 중심의 설계와 대규모 학습 데이터 세트가 필요한 작업별 신경망(NN)에 의존합니다.

## 20. VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots
- **Authors:** Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov
- **URL:** https://arxiv.org/abs/2507.05118
- **요약 (영문):** we propose an architecture for automatically verifying high-level task plans before their execution in simulators or real-world environments . the approach consists of two key steps: first, the conversion of natu .
- **요약 (한글):** 우리는 시뮬레이터 또는 실제 환경에서 실행하기 전에 높은 수준의 작업 계획을 자동으로 검증하는 아키텍처를 제안합니다. 이 접근 방식은 두 가지 주요 단계로 구성됩니다: 첫째, natu의 변환 .

## 21. PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs
- **Authors:** Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang
- **URL:** https://arxiv.org/abs/2507.05101
- **요약 (영문):** existing benchmarks focus on isolated pairwise evaluations . a model's capability to reconstruct biologically meaningful PPI networks is crucial .
- **요약 (한글):** 기존 벤치마크는 고립된 쌍별 평가에 초점을 맞추고 있습니다. 생물학적으로 의미 있는 PPI 네트워크를 재구성하는 모델의 역량이 중요합니다.

## 22. The Hidden Threat in Plain Text: Attacking RAG Data Loaders
- **Authors:** Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, Simeone Pizzi
- **URL:** https://arxiv.org/abs/2507.05093
- **요약 (영문):** large language models (LLMs) have transformed human-machine interaction since chatGPT's 2022 debut . reliance on ingesting external documents introduces new vulnerabilities . this paper exposes a critical security gap at the data loading stage .
- **요약 (한글):** 대규모 언어 모델(LLM)은 채팅GPT의 2022년 데뷔 이후 인간과 기계의 상호작용을 변화시켰습니다. 외부 문서 수집에 의존하면 새로운 취약점이 발생합니다. 이 백서에서는 데이터 로드 단계에서 중요한 보안 격차를 노출합니다.

## 23. Replacing thinking with tool usage enables reasoning in small language models
- **Authors:** Corrado Rainone, Tim Bakker, Roland Memisevic
- **URL:** https://arxiv.org/abs/2507.05065
- **요약 (영문):** a combination of supervised fine-tuning and Reinforcement Learning with Verifiable Rewards is used for training Large Language Models to expend extra compute during inference . in this paper, we propose to format these tokens as a multi-turn inte .
- **요약 (한글):** 이 논문에서는 추론 중에 추가 컴퓨팅을 소비하기 위해 대규모 언어 모델을 훈련하는 데 감독 미세 조정과 검증 가능한 보상이 있는 강화 학습의 조합을 사용하여 이러한 토큰을 멀티 턴 인테로 포맷할 것을 제안합니다.

## 24. INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling
- **Authors:** Xin Dong, Shichao Dong, Jin Wang, Jing Huang, Li Zhou, Zenghui Sun, Lihua Jing, Jingsong Lan, Xiaoyong Zhu, Bo Zheng
- **URL:** https://arxiv.org/abs/2507.05056
- **요약 (영문):** LVLMs may generate responses that appear plausible, but remain inconsistent with the associated visual content . this discrepancy arises from humans' ability to leverage multimodal interaction information .
- **요약 (한글):** LVLM은 그럴듯해 보이지만 관련 시각적 콘텐츠와 일치하지 않는 반응을 생성할 수 있으며, 이러한 불일치는 멀티모달 상호작용 정보를 활용하는 인간의 능력에서 비롯됩니다.

## 25. Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision
- **Authors:** Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy
- **URL:** https://arxiv.org/abs/2507.05020
- **요약 (영문):** traditional models lack flexibility, requiring a separate model for each . a key challenge in multi-task learning is multitask learning .
- **요약 (한글):** 기존 모델은 유연성이 부족하여 각각에 대해 별도의 모델이 필요합니다. 멀티태스크 학습의 핵심 과제는 멀티태스크 학습입니다.

## 26. LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning
- **Authors:** Sandipan Dhar, Mayank Gupta, Preeti Rao
- **URL:** https://arxiv.org/abs/2507.04966
- **요약 (영문):** the field of Singing Voice Synthesis (SVS) has seen significant advancements in recent years due to the rapid progress of diffusion-based approaches . however, capturing vocal style, genre-specific pitch inflections, and language-dependent characteristics remains challenging, particularly in low-resource scenarios . to address this, we propose a vocal-style guided learning mechanism, specifically designed for Bollywood Hindi singing st .
- **요약 (한글):** 노래 음성 합성(SVS) 분야는 최근 몇 년 동안 확산 기반 접근법의 빠른 발전으로 인해 상당한 발전을 보였습니다. 그러나 보컬 스타일, 장르별 음정 굴절 및 언어 의존적 특성을 캡처하는 것은 특히 리소스가 적은 시나리오에서 여전히 어려운 과제입니다. 이를 해결하기 위해 볼리우드 힌디어 노래를 위해 특별히 설계된 보컬 스타일 가이드 학습 메커니즘을 제안합니다.

## 27. HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding
- **Authors:** Yuxuan Cai, Jiangning Zhang, Zhenye Gan, Qingdong He, Xiaobin Hu, Junwei Zhu, Yabiao Wang, Chengjie Wang, Zhucun Xue, Xinwei He, Xiang Bai
- **URL:** https://arxiv.org/abs/2507.04909
- **요약 (영문):** multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks involving both images and videos . however, their capacity to comprehend human-centric video data remains underexplored .
- **요약 (한글):** 다중 모드 대규모 언어 모델(MLLM)은 이미지와 동영상을 모두 포함하는 시각적 이해 작업에서 상당한 발전을 보여 왔지만, 인간 중심의 비디오 데이터를 이해하는 능력은 여전히 미개척 분야로 남아 있습니다.

## 28. Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations
- **Authors:** A. Bochkov
- **URL:** https://arxiv.org/abs/2507.04886
- **요약 (영문):** the dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors" this paper challenges that view . we construct Transformer models where the embedded layer is entirely frozen .
- **요약 (한글):** 지배적인 패러다임은 훈련 가능한 입력 임베딩이 기초적인 '의미 벡터' 역할을 한다고 가정하는데, 이 논문은 이러한 관점에 도전합니다. 임베딩 계층이 완전히 고정된 트랜스포머 모델을 구축합니다.

## 29. A Survey of Pun Generation: Datasets, Evaluations and Methodologies
- **Authors:** Yuchen Su, Yonghua Zhu, Ruofan Wang, Zijian Huang, Diana Benavides-Prado, Michael Witbrock
- **URL:** https://arxiv.org/abs/2507.04793
- **요약 (영문):** pun generation seeks to create humour or evoke double meanings . it also aims to preserve coherence and contextual appropriateness . there is currently no dedicated survey that reviews this specific area .
- **요약 (한글):** 말장난 생성은 유머를 만들거나 이중 의미를 불러 일으키려고합니다. 또한 일관성과 문맥의 적절성을 유지하는 것을 목표로합니다. 현재이 특정 영역을 검토하는 전용 설문 조사는 없습니다.

## 30. From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection
- **Authors:** Zexi Jia, Chuanwei Huang, Yeshuang Zhu, Hongyan Fei, Ying Deng, Zhiqiang Yuan, Jiapei Zhang, Jinchao Zhang, Jie Zhou
- **URL:** https://arxiv.org/abs/2507.04769
- **요약 (영문):** current legal frameworks consider AI-generated works eligible for copyright protection when they meet originality requirements and involve substantial human intellectual input . systematic legal standards and reliable evaluation methods for AI art copyrights are lacking .
- **요약 (한글):** 현행 법체계는 인공지능이 생성한 저작물이 독창성 요건을 충족하고 인간의 지적 투입이 상당 부분 포함된 경우 저작권 보호 대상으로 간주하고 있으나, 인공지능 예술 저작권에 대한 체계적인 법적 기준과 신뢰할 수 있는 평가 방법이 부족합니다.

## 31. CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering
- **Authors:** Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, Enhong Chen
- **URL:** https://arxiv.org/abs/2507.04756
- **요약 (영문):** existing methods often rely on centralized fine-tuning or static preference alignment . large cloud-based models lack access to localized user-specific information .
- **요약 (한글):** 기존 방식은 중앙 집중식 미세 조정 또는 정적 환경 설정 조정에 의존하는 경우가 많으며, 대규모 클라우드 기반 모델은 현지화된 사용자별 정보에 대한 액세스가 부족합니다.

## 32. Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions
- **Authors:** Shuo Yang, Xinran Zheng, Xinchen Zhang, Jinfeng Xu, Jinze Li, Donglin Xie, Weicai Long, Edith C.H. Ngai
- **URL:** https://arxiv.org/abs/2507.04752
- **요약 (영문):** large language models (LLMs) have revolutionized various fields with their exceptional capabilities in understanding, processing, and generating human-like text . this paper investigates the potential of LLMs in advancing NIDS .
- **요약 (한글):** 대규모 언어 모델(LLM)은 인간과 유사한 텍스트를 이해하고 처리하며 생성하는 탁월한 능력으로 다양한 분야에 혁신을 가져왔습니다. 이 백서에서는 NIDS를 발전시키는 데 있어 LLM의 잠재력을 조사합니다.

## 33. Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems
- **Authors:** Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu
- **URL:** https://arxiv.org/abs/2507.04724
- **요약 (영문):** multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate remarkable capabilities in collaborative problem-solving . we design four representative attack paradigms that subtly disrupt task completion while maintaining high concealment.
- **요약 (한글):** 대규모 언어 모델(LLM-MAS)로 구동되는 멀티 에이전트 시스템은 협업 문제 해결에 탁월한 역량을 발휘하며, 높은 은폐성을 유지하면서 작업 완료를 교묘하게 방해하는 네 가지 대표적인 공격 패러다임을 설계합니다.

## 34. UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization
- **Authors:** Kai Yang, Zelin Zhu, Chengtao Jian, Hui Ma, Shengjie Zhao, Xiaozhou Ye, Ye Ouyang
- **URL:** https://arxiv.org/abs/2507.04706
- **요약 (영문):** central to UrbanMind is a novel architecture based on Continual Retrieval-Augmented MoE-based LLM (C-RAG-LLM) which dynamically incorporates domain-specific knowledge and evolving urban data to support long-term adaptabilities .
- **요약 (한글):** UrbanMind의 핵심은 도메인별 지식과 진화하는 도시 데이터를 동적으로 통합하여 장기적인 적응력을 지원하는 지속적 검색 증강 MoE 기반 LLM(C-RAG-LLM)을 기반으로 하는 새로운 아키텍처입니다.

## 35. Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning
- **Authors:** Feng Yue, Zhaoxing Zhang, Junming Jiao, Zhengyu Liang, Shiwen Cao, Feifei Zhang, Rong Shen
- **URL:** https://arxiv.org/abs/2507.04702
- **요약 (영문):** videos often have a larger volume of information and redundancy than texts or images . we propose Tempo-R0: a Video Multimodal Large Language Model (Video-MLLM) for the temporal video grounding.
- **요약 (한글):** 비디오는 텍스트나 이미지보다 정보의 양이 많고 중복되는 경우가 많기 때문에 시간적 비디오 접지를 위한 비디오 멀티모달 대용량 언어 모델(Video-MLLM)인 Tempo-R0을 제안합니다.

## 36. Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation
- **Authors:** Wenhao Li, Xiu Su, Jingyi Wu, Feng Yang, Yang Liu, Yi Chen, Shan You, Chang Xu
- **URL:** https://arxiv.org/abs/2507.04680
- **요약 (영문):** large vision-language models have demonstrated remarkable advancements in numerous areas such as multimedia . however, hallucination issues significantly limit their credibility and application potential .
- **요약 (한글):** 대형 비전 언어 모델은 멀티미디어와 같은 다양한 분야에서 괄목할 만한 발전을 보여 왔지만, 환각 문제로 인해 신뢰성과 적용 가능성이 크게 제한되고 있습니다.

## 37. Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs
- **Authors:** Swayamjit Saha
- **URL:** https://arxiv.org/abs/2507.04625
- **요약 (영문):** large language models (LLMs) are powerful yet prone to generating factual errors, commonly referred to as hallucinations . our method post-processes model outputs and corrects factual inconsistencies via external semantic memory .
- **요약 (한글):** 대규모 언어 모델(LLM)은 강력하지만 흔히 환각이라고 불리는 사실 오류를 발생시키기 쉽습니다. 우리의 방법은 모델 출력을 후처리하고 외부 시맨틱 메모리를 통해 사실 불일치를 수정합니다.

## 38. Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation
- **Authors:** Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji
- **URL:** https://arxiv.org/abs/2507.04623
- **요약 (영문):** Existing SBR models often focus only on single-session information . some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance .
- **요약 (한글):** 기존 SBR 모델은 단일 세션 정보에만 초점을 맞추는 경우가 많으며, 일부 방법은 세션 간 데이터를 포함하려고 시도하지만 노이즈와 관련 없는 정보로 인해 성능이 저하되는 문제가 있습니다.

## 39. Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences
- **Authors:** Yusong Zhang, Yuxuan Sun, Lei Guo, Wei Chen, Bo Ai, Deniz Gunduz
- **URL:** https://arxiv.org/abs/2507.04621
- **요약 (영문):** 6G networks promise revolutionary communication experiences including augmented reality (AR), virtual reality (VR) and holographic communications . these applications require high-dimensional multimodal data transmission and intelligent data processing in real-time .
- **요약 (한글):** 6G 네트워크는 증강현실(AR), 가상현실(VR), 홀로그램 통신 등 혁신적인 통신 경험을 약속합니다. 이러한 애플리케이션에는 고차원 멀티모달 데이터 전송과 실시간 지능형 데이터 처리가 필요합니다.

## 40. any4: Learned 4-bit Numeric Representation for LLMs
- **Authors:** Mostafa Elhoushi, Jeff Johnson
- **URL:** https://arxiv.org/abs/2507.04610
- **요약 (영문):** any4 is a learned 4-bit weight quantization solution for large language models . it provides arbitrary numeric representations without pre-processing of weights or activations .
- **요약 (한글):** any4는 대규모 언어 모델을 위한 학습된 4비트 가중치 양자화 솔루션으로, 가중치나 활성화의 사전 처리 없이 임의의 숫자 표현을 제공합니다.

## 41. PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes
- **Authors:** Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang
- **URL:** https://arxiv.org/abs/2507.04607
- **요약 (영문):** large language model personalization aims to align model outputs with individual's unique preferences and opinions . we integrate the well-established cognitive dual-memory model into LLM personalization by mirroring episodic memory to historical user engagements .
- **요약 (한글):** 대규모 언어 모델 개인화는 모델 결과물을 개인의 고유한 선호도 및 의견에 맞추는 것을 목표로 하며, 잘 확립된 인지 이중 메모리 모델을 과거 사용자 참여에 에피소드 메모리를 미러링하여 LLM 개인화에 통합합니다.
