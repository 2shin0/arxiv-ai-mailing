from crawler import fetch_recent_ai_papers
from summarizer import summarize_text, translate_text
from email_sender import send_email
from datetime import datetime
import re
import os
import json

LLM_PATTERN = re.compile(r'\b(llm|language model(?:s)?|(?:^|[^a-zA-Z])lm(?:$|[^a-zA-Z]))\b')

def make_digest(papers):
    today = datetime.today().strftime('%Y-%m-%d')
    lines = [f"<strong>arXiv cs.AI ë…¼ë¬¸ ìš”ì•½ Digest - {today}</strong><br><br>"]

    llm_papers = []
    other_papers = []

    for paper in papers:
        combined_text = (paper['title'] + ' ' + paper['abstract'])
        if LLM_PATTERN.search(combined_text):
            llm_papers.append(paper)
        else:
            other_papers.append(paper)

    llm_papers = llm_papers[:5]
    other_papers = other_papers[:5]

    if llm_papers:
        lines.append("<h3>ğŸ” LLM ê´€ë ¨ ë…¼ë¬¸</h3>")
        for i, paper in enumerate(llm_papers, 1):
            summary_en = summarize_text(paper['abstract'])
            summary_ko = translate_text(summary_en)
            lines.append(f"<strong>{i}. {paper['title']}</strong>")
            lines.append(f"- Authors: {paper['authors']}")
            lines.append(f"- URL: <a href='{paper['url']}'>{paper['url']}</a>")
            lines.append(f"- ìš”ì•½ (ì˜ë¬¸): {summary_en}")
            lines.append(f"- ìš”ì•½ (í•œê¸€): {summary_ko}<br><br>")
        lines.append("<a href='https://2shin0.tistory.com/14' target='_blank'>ğŸ“ LLM ë…¼ë¬¸ ëª¨ë‘ ë³´ê¸°</a><br><br>")

    if other_papers:
        lines.append("<h3>ğŸ“š ê·¸ ì™¸ ë…¼ë¬¸</h3>")
        for i, paper in enumerate(other_papers, 1):
            summary_en = summarize_text(paper['abstract'])
            summary_ko = translate_text(summary_en)
            lines.append(f"<strong>{i}. {paper['title']}</strong>")
            lines.append(f"- Authors: {paper['authors']}")
            lines.append(f"- URL: <a href='{paper['url']}'>{paper['url']}</a>")
            lines.append(f"- ìš”ì•½ (ì˜ë¬¸): {summary_en}")
            lines.append(f"- ìš”ì•½ (í•œê¸€): {summary_ko}<br><br>")

    lines.append("<a href='https://2shin0.tistory.com/14' target='_blank'>ğŸ“š ì „ì²´ ë…¼ë¬¸ ë³´ëŸ¬ê°€ê¸°</a>")

    return '\n'.join(lines)

def main():
    papers = fetch_recent_ai_papers()
    if not papers:
        print("ì–´ì œ ë“±ë¡ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    today_str = datetime.today().strftime('%y%m%d')
    results_dir = "results"
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    
    file_path = os.path.join(results_dir, f"arxiv_{today_str}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=4)
    print(f"í¬ë¡¤ë§ëœ ë…¼ë¬¸ {len(papers)}ê°œê°€ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    digest = make_digest(papers)
    
    # ì´ë©”ì¼ ì „ì†¡ ì‹œë„ ë° ê²°ê³¼ ì²˜ë¦¬
    email_result = send_email("[arXiv AI Digest] ì–´ì œì˜ ë…¼ë¬¸ ìš”ì•½", digest)
    
    # ì´ë©”ì¼ ì „ì†¡ ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬
    if email_result:
        print("ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ì´ë©”ì¼ ì „ì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ì‹¤íŒ¨ ì‹œ ë‹¤ì´ì œìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open(f"arxiv_digest_{today_str}.txt", "w", encoding="utf-8") as f:
            f.write(digest)
        print(f"ë‹¤ì´ì œìŠ¤íŠ¸ê°€ íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: arxiv_digest_{today_str}.txt")

if __name__ == "__main__":
    main()
