from crawler import fetch_recent_ai_papers
from summarizer import summarize_text, translate_text
from email_sender import send_email
from datetime import datetime
import re
import os
import json
import subprocess
from exporter import generate_digest_markdown, update_all_indexes

LLM_PATTERN = re.compile(r'\b(llm|language model(?:s)?|(?:^|[^a-zA-Z])lm(?:$|[^a-zA-Z]))\b', re.IGNORECASE)

def make_digest(papers):
    today = datetime.today().strftime('%Y-%m-%d')
    lines = [f"<strong>arXiv cs.AI ë…¼ë¬¸ ìš”ì•½ Digest - {today}</strong><br><br>"]

    llm_papers = []
    other_papers = []

    for paper in papers:
        combined_text = (paper['title'] + ' ' + paper['abstract'])
        if LLM_PATTERN.search(combined_text):
            llm_papers.append(paper)
        else:
            other_papers.append(paper)

    llm_papers = llm_papers[:5]
    other_papers = other_papers[:5]

    if llm_papers:
        lines.append("<h3>ğŸ” LLM ê´€ë ¨ ë…¼ë¬¸</h3>")
        for i, paper in enumerate(llm_papers, 1):
            summary_en = summarize_text(paper['abstract'])
            summary_ko = translate_text(summary_en)
            lines.append(f"<strong>{i}. {paper['title']}</strong>")
            lines.append(f"- Authors: {paper['authors']}")
            lines.append(f"- URL: <a href='{paper['url']}'>{paper['url']}</a>")
            lines.append(f"- ìš”ì•½ (ì˜ë¬¸): {summary_en}")
            lines.append(f"- ìš”ì•½ (í•œê¸€): {summary_ko}<br><br>")
        llm_url = f"https://2shin0.github.io/arxiv-ai-mailing/LLM/{today}"
        lines.append(f'<a href="{llm_url}" target="_blank" style="display: inline-block; padding: 10px 15px; font-size: 14px; color: #fff; background-color: #007bff; text-decoration: none; border-radius: 5px;">ğŸ“ LLM ë…¼ë¬¸ ëª¨ë‘ ë³´ê¸°</a><br><br>')

    if other_papers:
        lines.append("<h3>ğŸ“š ê·¸ ì™¸ ë…¼ë¬¸</h3>")
        for i, paper in enumerate(other_papers, 1):
            summary_en = summarize_text(paper['abstract'])
            summary_ko = translate_text(summary_en)
            lines.append(f"<strong>{i}. {paper['title']}</strong>")
            lines.append(f"- Authors: {paper['authors']}")
            lines.append(f"- URL: <a href='{paper['url']}'>{paper['url']}</a>")
            lines.append(f"- ìš”ì•½ (ì˜ë¬¸): {summary_en}")
            lines.append(f"- ìš”ì•½ (í•œê¸€): {summary_ko}<br><br>")
        all_url = f"https://2shin0.github.io/arxiv-ai-mailing/ALL/{today}"
        lines.append(f'<a href="{all_url}" target="_blank" style="display: inline-block; padding: 10px 15px; font-size: 14px; color: #fff; background-color: #28a745; text-decoration: none; border-radius: 5px;">ğŸ“š ì „ì²´ ë…¼ë¬¸ ë³´ëŸ¬ê°€ê¸°</a>')

    return '\n'.join(lines)

def main():
    # 0. ë‹¹ì¼ ì´ë¯¸ ë©”ì¼ì´ ë°œì†¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
    today_str = datetime.today().strftime('%Y-%m-%d')
    email_flag_file = f"results/{today_str}_email_sent.flag"
    
    if os.path.exists(email_flag_file):
        print(f"ì˜¤ëŠ˜({today_str}) ì´ë¯¸ ë©”ì¼ì´ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        print(f"ìƒíƒœ íŒŒì¼: {email_flag_file}")
        return
    
    # 1. ì–´ì œì ë…¼ë¬¸ í¬ë¡¤ë§
    papers = fetch_recent_ai_papers()
    if not papers:
        print("ì–´ì œ ë“±ë¡ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ (JSON ë°±ì—…)
    today_str = datetime.today().strftime('%y%m%d')
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    file_path = os.path.join(results_dir, f"arxiv_{today_str}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=4)
    print(f"í¬ë¡¤ë§ëœ ë…¼ë¬¸ {len(papers)}ê°œê°€ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 3. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„± (GitHub ë°°í¬ìš©)
    print("ë§ˆí¬ë‹¤ìš´ ë‹¤ì´ì œìŠ¤íŠ¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    llm_papers = [
        p for p in papers
        if LLM_PATTERN.search(p['title'] + ' ' + p['abstract'])
    ]
    today_for_md = datetime.today().strftime('%Y-%m-%d')
    llm_md_path = os.path.join('digest', 'LLM', f"{today_for_md}.md")
    all_md_path = os.path.join('digest', 'ALL', f"{today_for_md}.md")
    generate_digest_markdown(llm_papers, llm_md_path, "LLM ê´€ë ¨ ì£¼ìš” ë…¼ë¬¸")
    generate_digest_markdown(papers, all_md_path, "ì „ì²´ AI ë…¼ë¬¸")

    # 3.1. ì¸ë±ìŠ¤ í˜ì´ì§€ ì—…ë°ì´íŠ¸
    print("ì¸ë±ìŠ¤ í˜ì´ì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤...")
    update_all_indexes()

    # # 4. GitHubìœ¼ë¡œ ë‹¤ì´ì œìŠ¤íŠ¸ ë°°í¬
    # print("GitHubìœ¼ë¡œ ë‹¤ì´ì œìŠ¤íŠ¸ ë°°í¬ ì¤‘...")
    # try:
    #     subprocess.run(
    #         "bash ./deploy_digest.sh",
    #         check=True,
    #         shell=True
    #     )
    #     print("ë°°í¬ ì„±ê³µ!")
    # except subprocess.CalledProcessError as e:
    #     print(f"ë°°í¬ ì‹¤íŒ¨: ìŠ¤í¬ë¦½íŠ¸ê°€ ì˜¤ë¥˜ ì½”ë“œ {e.returncode}(ìœ¼)ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    #     print("ë°°í¬ì— ì‹¤íŒ¨í•˜ì—¬ ì´ë©”ì¼ ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    #     return # ë°°í¬ ì‹¤íŒ¨ ì‹œ ì—¬ê¸°ì„œ ì¤‘ë‹¨

    # # 5. ì´ë©”ì¼ ë³¸ë¬¸ ìƒì„± ë° ì „ì†¡
    # print("ì´ë©”ì¼ ì „ì†¡ì„ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    # digest_html = make_digest(papers) # ìš”ì•½/ë²ˆì—­ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì´ë©”ì¼ ë³¸ë¬¸ ìƒì„±
    # email_result = send_email("[LLMxiv] ë°ì¼ë¦¬ ë…¼ë¬¸ ë¦¬ë·°", digest_html)

    # if email_result:
    #     print("ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    #     # ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ í›„ ìƒíƒœ íŒŒì¼ ìƒì„±
    #     with open(email_flag_file, "w") as f:
    #         f.write("Email sent successfully.")
    # else:
    #     print("ì´ë©”ì¼ ì „ì†¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    #     # ì‹¤íŒ¨ ì‹œ ë‹¤ì´ì œìŠ¤íŠ¸ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥
    #     with open(f"results_html/arxiv_digest_{today_str}.html", "w", encoding="utf-8") as f:
    #         f.write(digest_html)
    #     print(f"ë‹¤ì´ì œìŠ¤íŠ¸ê°€ HTML íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: results_html/arxiv_digest_{today_str}.html")

if __name__ == "__main__":
    main()
